{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve, train_test_split, validation_curve\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "#change working directory to the location of the data file\n",
    "os.chdir('/mnt/d/Sajjad/08-2023/Python Code/Introduction to Machine Learning/')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"default_cc.csv\")\n",
    "\n",
    "# Changed Gender from int64 to str\n",
    "df1['Gender'] = df1['Gender'].astype('str')\n",
    "\n",
    "# Changed EDUCATION from int64 to str\n",
    "df1['EDUCATION'] = df1['EDUCATION'].astype('str')\n",
    "\n",
    "# Changed MARRIAGE from int64 to str\n",
    "df1['MARRIAGE'] = df1['MARRIAGE'].astype('str')\n",
    "\n",
    "# Deleted column ID from df1\n",
    "df1.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df1)\n",
    "X = df_onehot.loc[:, df_onehot.columns != 'default payment next month']\n",
    "y = df_onehot[['default payment next month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve for Training and Testing/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe_lr = make_pipeline(StandardScaler(),\n",
    "#                        LogisticRegression(penalty='l2', max_iter=10000))\n",
    "\n",
    "pipe_lr = make_pipeline(DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0))\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=pipe_lr,\n",
    "                               X=X_train,\n",
    "                               y=y_train,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                               cv=10,\n",
    "                               n_jobs=1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.8, 1.03])\n",
    "plt.tight_layout()\n",
    "# plt.savefig('figures/06_05.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, model_name):\n",
    "    model.fit(trainX,trainy)\n",
    "    md_probs = model.predict_proba(testX)\n",
    "    md_probs = md_probs[:,1]\n",
    "    md_auc = roc_auc_score(testy, md_probs)\n",
    "    print(model_name, \" : \", md_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_models = 300\n",
    "depth_level = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest and Graident Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(max_depth=10,n_estimators=num_of_models)\n",
    "fit_model(rf, \"Random Forest\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time RF: \", total_time)\n",
    "\n",
    "#record the start time\n",
    "start_time = time.time()\n",
    "gb = GradientBoostingClassifier(max_depth=depth_level,n_estimators=num_of_models)\n",
    "fit_model(gb, \"Graident Boosting\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time GB: \", total_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(max_depth=10,n_estimators=num_of_models, n_jobs=-1 )\n",
    "fit_model(rf, \"Random Forest\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time RF: \", total_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "ab = AdaBoostClassifier(n_estimators=num_of_models)\n",
    "fit_model(ab, \"Adaptive Boosting\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time AB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging using Categorical Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_categorical(df1):\n",
    "    df_q = pd.DataFrame()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in df1:\n",
    "        if col not in ['Gender','EDUCATION','MARRIAGE','default payment next month']:\n",
    "            df_q[col] = pd.qcut(df1[col], 5, duplicates='drop')            \n",
    "            df_q[col]= label_encoder.fit_transform(df_q[col])\n",
    "            df_q[col] = df_q[col].astype('str')\n",
    "\n",
    "    X_cat = df1[['Gender','EDUCATION','MARRIAGE']]\n",
    "    df_cat = pd.concat([df_q,X_cat],axis=1)\n",
    "    return df_cat\n",
    "\n",
    " \n",
    "temp_df1 = convert_categorical(df1) \n",
    "temp_df1.head()\n",
    "\n",
    "X_cat = convert_categorical(df1)\n",
    "trainX, testX, trainy, testy = train_test_split(X_cat, y, test_size=0.3, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "nb_c = CategoricalNB()\n",
    "bg_c = BaggingClassifier(base_estimator=nb_c, n_estimators=num_of_models, n_jobs=-1)\n",
    "fit_model(ab, \"Bagging Classifier using NB\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time AB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use xgboost\n",
    "xgb_model = xgb.XGBClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "xgb_model.fit(trainX,trainy)\n",
    "md_probs = xgb_model.predict_proba(testX)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy, md_probs)\n",
    "print(\"XG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time XGB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use lgboost\n",
    "lgb_model = lgb.LGBMClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "lgb_model.fit(trainX,trainy)\n",
    "md_probs = lgb_model.predict_proba(testX)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy, md_probs)\n",
    "print(\"LG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time LGB: \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = df1.loc[:, df1.columns != 'default payment next month']\n",
    "y2 = df1[['default payment next month']]\n",
    "trainX2, testX2, trainy2, testy2 = train_test_split(X2, y2, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = CatBoostClassifier(iterations=num_of_models, depth=depth_level, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "#record the start time\n",
    "start_time = time.time()\n",
    "cb.fit(trainX2,trainy2)\n",
    "md_probs = cb.predict_proba(testX2)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy2, md_probs)\n",
    "print(\"Cat Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time CB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(lgb_model, figsize=(10, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgb_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
